import torch
import wandb
import csv
import os
from datetime import datetime
'''
def mkdirs(path):
    if not os.path.exists(path):
        os.makedirs(path)
    return path

class Logger(object):
    """ Adapted from https://github.com/snap-stanford/ogb/ """
    def __init__(self, runs, args=None):
        self.args = args
        self.results = [[] for _ in range(runs)]

    def add_result(self, run, result):
        assert len(result) == 7
        assert run >= 0 and run < len(self.results)
        self.results[run].append(result)

    @staticmethod
    def get_results_string(best_result):
        result_string = ''
        r = best_result[:, 0]
        result_string += f'Highest Train: {r.mean():.2f} Â± {r.std():.2f}\t'
        r = best_result[:, 1]
        result_string += f'Highest Test: {r.mean():.2f} Â± {r.std():.2f}\t'
        r = best_result[:, 2]
        result_string += f'Highest Valid: {r.mean():.2f} Â± {r.std():.2f}\t'
        r = best_result[:, 3]
        result_string += f'  Final Train: {r.mean():.2f} Â± {r.std():.2f}\t'
        r = best_result[:, 4]
        result_string += f'   Final Test: {r.mean():.2f} Â± {r.std():.2f}'

        return result_string

    def print_statistics(self, run=None, mode='max_acc'):
        if run is not None:
            # Ensure all elements are tensors and convert them properly
            result = [torch.tensor(r) * 100 if isinstance(r, (float, int)) else torch.tensor(r) * 100 for r in self.results[run]]
            result = torch.stack(result)  # Stack the list of tensors into a single tensor

            if self.args.save_whole_test_result:
                now = datetime.now()
                _month_day = now.strftime("%m%d")
                timestamp = now.strftime("%m%d-%H%M%S")
                results_path = mkdirs(f'results/runs/{self.args.dataset}/{_month_day}/{self.args.wandb_name}')
                with open(f'{results_path}/{run}-{self.args.run_id}-results.csv', 'w', newline='') as f:
                    writer = csv.writer(f)
                    # Write the header (optional)
                    writer.writerow(["Epoch", "Train Acc", "Val Acc", "Test Acc", "Val Loss"])

                    # Write the data
                    for epoch in range(len(self.results[run])):
                        # Add 1 to run and epoch indices to match with human counting
                        formatted_row = ['{:.4f}'.format(float(x)) for x in self.results[run][epoch]]
                        writer.writerow([epoch * self.args.eval_step] + formatted_row)
                    # Write the args
                    writer.writerow([])
                    writer.writerow(["Args"])
                    for key, value in vars(self.args).items():
                        writer.writerow([key, value])

                    print(f"Saved results to {self.args.dataset}-{self.args.wandb_name}-{run}-results.csv")

            argmax = result[:, 1].argmax().item()
            argmin = result[:, 3].argmin().item()
            if mode == 'max_acc':
                ind = argmax
            else:
                ind = argmin
            print('==========================')
            print_str1 = f'>> Run {run + 1:02d}:\n' + \
                        f'\t Highest Train: {result[:, 0].max():.2f} ' + \
                        f'\t Highest Valid: {result[:, 1].max():.2f} ' + \
                        f'\t Highest Test: {result[:, 2].max():.2f}\n' + \
                        f'\t Chosen epoch based on Valid loss: {argmin * self.args.eval_step} ' + \
                        f'\t Final Train: {result[argmin, 0]:.2f} ' + \
                        f'\t Final Valid: {result[argmin, 1]:.2f} ' + \
                        f'\t Final Test: {result[argmin, 2]:.2f}'
            print(print_str1)

            print_str=f'>> Run {run + 1:02d}:' + \
                f'\t Highest Train: {result[:, 0].max():.2f} ' + \
                f'\t Highest Valid: {result[:, 1].max():.2f} ' + \
                f'\t Highest Test: {result[:, 2].max():.2f}\n' + \
                f'\t Chosen epoch based on Valid acc: {ind * self.args.eval_step} ' + \
                f'\t Final Train: {result[ind, 0]:.2f} ' + \
                f'\t Final Valid: {result[ind, 1]:.2f} ' + \
                f'\t Final Test: {result[ind, 2]:.2f}'
            print(print_str)
            self.test = result[ind, 2]
        else:
            best_results = []
            max_val_epoch = 0

            for r in self.results:
                r = [torch.tensor(res) * 100 if isinstance(res, (float, int)) else torch.tensor(res) * 100 for res in r]
                r = torch.stack(r)  # Stack the list of tensors into a single tensor
                train1 = r[:, 0].max().item()
                test1 = r[:, 2].max().item()
                valid = r[:, 1].max().item()
                if mode == 'max_acc':
                    train2 = r[r[:, 1].argmax(), 0].item()
                    test2 = r[r[:, 1].argmax(), 2].item()
                    max_val_epoch = r[:, 1].argmax()
                else:
                    train2 = r[r[:, 3].argmin(), 0].item()
                    test2 = r[r[:, 3].argmin(), 2].item()
                best_results.append((train1, test1, valid, train2, test2))

            best_result = torch.tensor(best_results)

            print(f'All runs:')
            r = best_result[:, 0]
            print(f'Highest Train: {r.mean():.2f} Â± {r.std():.2f}')
            r = best_result[:, 1]
            print(f'Highest Test: {r.mean():.2f} Â± {r.std():.2f}')
            r = best_result[:, 2]
            print(f'Highest Valid: {r.mean():.2f} Â± {r.std():.2f}')
            r = best_result[:, 3]
            print(f'  Final Train: {r.mean():.2f} Â± {r.std():.2f}')
            r = best_result[:, 4]
            print(f'   Final Test: {r.mean():.2f} Â± {r.std():.2f}')

            self.test = r.mean()
            # if self.args.use_wandb:
            #     wandb.log({
            #         'Average Highest Train': r.mean().item(),
            #         'Std Highest Train': r.std().item(),
            #         'Average Highest Test': best_result[:, 1].mean().item(),
            #         'Std Highest Test': best_result[:, 1].std().item(),
            #         'Average Highest Valid': best_result[:, 2].mean().item(),
            #         'Std Highest Valid': best_result[:, 2].std().item(),
            #         'Average Final Train': best_result[:, 3].mean().item(),
            #         'Std Final Train': best_result[:, 3].std().item(),
            #         'Average Final Test': best_result[:, 4].mean().item(),
            #         'Std Final Test': best_result[:, 4].std().item()
            #     })
            return self.get_results_string(best_result)

    def save(self, params, results, filename):
        with open(filename, 'a', encoding='utf-8') as file:
            file.write(f"{results}\n")
            file.write(f"{params}\n")
            file.write('=='*50)
            file.write('\n')
            file.write('\n')

import os
def save_result(args, results):
    if args.save_result:
        if not os.path.exists(f'results/{args.dataset}'):
            os.makedirs(f'results/{args.dataset}')
        filename = f'results/{args.dataset}/{args.method}.csv'
        print(f"Saving results to {filename}")
        with open(f"{filename}", 'a+') as write_obj:
            write_obj.write(
                f"{args.method} " + f"{args.kernel}: " + f"{args.weight_decay} " + f"{args.dropout} " + \
                f"{args.num_layers} " + f"{args.alpha}: " + f"{args.hidden_channels}: " + \
                f"{results.mean():.2f} $\pm$ {results.std():.2f} \n")
'''
# logger.py
import os
import csv
from datetime import datetime
import torch
import pandas as pd


def mkdirs(path):
    if not os.path.exists(path):
        os.makedirs(path)
    return path


class Logger(object):
    """
    è®°å½• & æ‰“å° & ä¿å­˜ 10 ä¸ªéªŒè¯/æµ‹è¯•æŒ‡æ ‡
        0  val_loss       5  test_loss
        1  val_acc        6  test_acc
        2  val_f1         7  test_f1
        3  val_prec       8  test_prec
        4  val_rec        9  test_rec
    """
    def append_run_to_excel(self, run: int, file_path: str,
                            select_by: str = 'val_acc') -> None:
        """
        å°†æŒ‡å®š run çš„æœ€ä½³æŒ‡æ ‡ï¼ˆ10 åˆ—ï¼‰è¿½åŠ å†™å…¥åŒä¸€ä¸ª Excelã€‚
        è‹¥ç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨åˆ™è¯»å–â†’å»é‡â†’æ‹¼æ¥â†’å†™å›ï¼›å¦åˆ™ç›´æ¥åˆ›å»ºã€‚
        """
        # ---------- å‡†å¤‡è¡Œæ•°æ® ----------
        # self.results[run] å½¢çŠ¶: [epochs, 10]
        metrics = torch.tensor(self.results[run])

        if select_by == 'val_acc':
            best_ep = metrics[:, 1].argmax()      # val_acc æœ€å¤§çš„ epoch
        elif select_by == 'test_acc':
            best_ep = metrics[:, 6].argmax()      # test_acc æœ€å¤§çš„ epoch
        else:
            raise ValueError("select_by ä»…æ”¯æŒ 'val_acc' æˆ– 'test_acc'")

        # æ„å»º DataFrameï¼ˆç¬¬ä¸€åˆ—è®°å½• run ç¼–å·ï¼‰
        row = [run] + metrics[best_ep].tolist()
        cols = [
            'run',
            'val/loss', 'val/acc', 'val/f1', 'val/precision', 'val/recall',
            'test/loss', 'test/acc', 'test/f1', 'test/precision', 'test/recall'
        ]
        df_new = pd.DataFrame([row], columns=cols)

        # ---------- è¿½åŠ å†™å…¥ ----------
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        if os.path.exists(file_path):
            # è¯»å–æ—§æ–‡ä»¶
            df_old = pd.read_excel(file_path)
            # é˜²æ­¢é‡å¤è¡Œï¼šå»é™¤åŒ run å·²æœ‰è®°å½•
            df_old = df_old[df_old['run'] != run]
            # æ‹¼æ¥å¹¶é‡ç½®ç´¢å¼•
            df_new = pd.concat([df_old, df_new], ignore_index=True)

        # è¦†ç›–å†™å›ï¼ˆç›¸å½“äºâ€œè¿½åŠ â€æ•ˆæœï¼‰
        df_new.to_excel(file_path, index=False)
        print(f'ğŸ“¥ å·²è¿½åŠ ä¿å­˜ run {run} â†’ {file_path}')
    def __init__(self, runs, args=None):
        self.args = args
        # results[run] = [ [metric10], [metric10], ... ]  æŒ‰ epoch å­˜
        self.results = [[] for _ in range(runs)]

    # ------------------------------------------------------------------
    # æ¯åˆ°ä¸€æ¬¡ eval_step è°ƒç”¨ï¼ŒæŠŠ 10 ä¸ªæŒ‡æ ‡ append
    # ------------------------------------------------------------------
    def add_result(self, run: int, result: list):
        """
        Parameters
        ----------
        run : ç¬¬å‡ ä¸ª run
        result : é•¿åº¦ 10 çš„ list / tupleï¼Œé¡ºåºè§ç±»æ³¨é‡Š
        """
        assert len(result) == 10, "Logger.add_result éœ€è¦ 10 ä¸ªæŒ‡æ ‡"
        assert 0 <= run < len(self.results)
        self.results[run].append([float(x) for x in result])

    # ------------------------------------------------------------------
    # æ‰“å°å•ä¸ª / æ‰€æœ‰ run ç»Ÿè®¡
    # ------------------------------------------------------------------
    @staticmethod
    def _tensorize(lst):
        return torch.tensor(lst) * 100.0      # ç™¾åˆ†æ¯”æ˜¾ç¤º

    def print_statistics(self, run=None, mode='max_acc'):
        """
        run=None  => æ‰“å°æ‰€æœ‰ run æ±‡æ€»
        run=int   => æ‰“å°è¯¥ run æœ€é«˜ / æœ€ä½³ epoch
        mode      => 'max_acc'(æŒ‰ val_acc æœ€å¤§é€‰ epoch) æˆ– 'min_loss'
        """
        # === å•ä¸ª run === ------------------------------------------------
        if run is not None:
            r = self._tensorize(self.results[run])        # shape [epochs, 10]
            # ä¾æ® val_acc æœ€å¤§æˆ– val_loss æœ€å°æŒ‘ epoch
            if mode == 'max_acc':
                best_epoch = r[:, 1].argmax().item()
            else:
                best_epoch = r[:, 0].argmin().item()

            print('==========================')
            print(f'>> Run {run + 1:02d}:')
            print(f'   â”œâ”€ Highest Val Acc : {r[:, 1].max():.2f}%  '
                  f'@ epoch {r[:, 1].argmax().item()}')
            print(f'   â””â”€ Selected Epoch  : {best_epoch}  '
                  f' Val Acc {r[best_epoch,1]:.2f}% | '
                  f' Test Acc {r[best_epoch,6]:.2f}%')
            # æ–¹ä¾¿ä¸»ç¨‹åºè®¿é—® test_acc
            self.test = r[best_epoch, 6]

            # è‹¥å¼€å¯ä¿å­˜æ¯è½®ç»“æœ
            if getattr(self.args, 'save_whole_test_result', False):
                self._dump_whole_run_csv(run)

        # === æ±‡æ€»å…¨éƒ¨ run === --------------------------------------------
        else:
            summary = []
            for r in self.results:
                r = self._tensorize(r)
                # æœ€é«˜ val_acc
                best_ep = r[:, 1].argmax()
                summary.append([
                    r[best_ep, 1].item(),    # val_acc
                    r[best_ep, 6].item()     # test_acc
                ])
            summary = torch.tensor(summary)   # [runs, 2]

            print('==========================')
            print('>> All Runs Summary (based on best Val Acc)')
            print(f'   Val Acc : {summary[:,0].mean():.2f} Â± {summary[:,0].std():.2f}%')
            print(f'   Test Acc: {summary[:,1].mean():.2f} Â± {summary[:,1].std():.2f}%')

            # å¯é€‰ï¼šæŠŠæ€»ç»“æœå†™è¿› WANDBã€CSV ç­‰
            return summary

    # ------------------------------------------------------------------
    # æ•´ä¸ª run æ‰€æœ‰ epoch çš„ 10 æŒ‡æ ‡ â†’ CSV
    # ------------------------------------------------------------------
    def _dump_whole_run_csv(self, run: int):
        now = datetime.now()
        stamp_dir = now.strftime("%m%d")
        timestamp = now.strftime("%m%d-%H%M%S")

        save_dir = mkdirs(
            f'results/runs/{self.args.dataset}/{stamp_dir}/{self.args.wandb_name}'
        )
        csv_path = f'{save_dir}/{run}-{self.args.run_id}-results.csv'

        header = [
            "Epoch",
            'val/loss', 'val/acc', 'val/f1', 'val/precision', 'val/recall',
            'test/loss', 'test/acc', 'test/f1', 'test/precision', 'test/recall'
        ]
        with open(csv_path, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(header)
            for idx, metrics in enumerate(self.results[run]):
                writer.writerow([idx * self.args.eval_step] + metrics)
            writer.writerow([]); writer.writerow(['Args'])
            for k, v in vars(self.args).items():
                writer.writerow([k, v])
        print(f'ğŸ“„ Saved per-epoch metrics â†’ {csv_path}')

    # ------------------------------------------------------------------
    # è®­ç»ƒå…¨éƒ¨å®Œæˆåï¼šä¸€æ¬¡æ€§å¯¼å‡º Excelï¼ˆæ¯ run ä¸€è¡Œï¼Œ10 åˆ—ï¼‰
    # å¤–éƒ¨è°ƒç”¨ï¼šlogger.save_to_excel("results/exp_metrics.xlsx")
    # ------------------------------------------------------------------
    def save_to_excel(self, file_path):
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        columns = [
            'val/loss', 'val/acc', 'val/f1', 'val/precision', 'val/recall',
            'test/loss', 'test/acc', 'test/f1', 'test/precision', 'test/recall'
        ]
        # å–å„ run â€œæœ€ä½³ val_acc epochâ€ çš„ 10 æŒ‡æ ‡
        rows = []
        for r in self.results:
            r = torch.tensor(r)
            best_ep = r[:, 1].argmax()
            rows.append(r[best_ep].tolist())
        df = pd.DataFrame(rows, columns=columns)
        df.to_excel(file_path, index=False)
        print(f'âœ… Excel saved : {file_path}')


# ----------------------------------------------------------------------
# ä¿ç•™ä½ åŸæ¥çš„ save_result()ï¼Œæ— éœ€ä¿®æ”¹
# ----------------------------------------------------------------------
def save_result(args, results):
    if args.save_result:
        os.makedirs(f'results/{args.dataset}', exist_ok=True)
        filename = f'results/{args.dataset}/{args.method}.csv'
        print(f"Saving results to {filename}")
        with open(filename, 'a+') as f:
            f.write(
                f"{args.method} {args.kernel}: {args.weight_decay} "
                f"{args.dropout} {args.num_layers} {args.alpha}: "
                f"{args.hidden_channels}: "
                f"{results.mean():.2f} Â± {results.std():.2f}\n"
            )
def save_to_excel(self, file_path):
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    columns = [
        'run',
        'val/loss', 'val/acc', 'val/f1', 'val/precision', 'val/recall',
        'test/loss', 'test/acc', 'test/f1', 'test/precision', 'test/recall'
    ]
    rows = []

    for run_id, r in enumerate(self.results):
        r = torch.tensor(r)
        best_ep = r[:, 6].argmax()  # åŸºäº test_acc æœ€å¤§
        row = [run_id] + r[best_ep].tolist()
        rows.append(row)

    df = pd.DataFrame(rows, columns=columns)

    # è‹¥å·²å­˜åœ¨åˆ™è¿½åŠ 
    if os.path.exists(file_path):
        old_df = pd.read_excel(file_path)
        df = pd.concat([old_df, df], ignore_index=True)

    df.to_excel(file_path, index=False)
    print(f'âœ… Excel å·²ä¿å­˜: {file_path}')
